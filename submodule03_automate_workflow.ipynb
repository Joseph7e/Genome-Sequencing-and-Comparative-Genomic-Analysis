{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e5d6a0-1bf7-4fa8-a981-b8f7da588f76",
   "metadata": {},
   "source": [
    "# Submodule 3: Workflow automation with Nextflow and aquisition of public datasets\n",
    "--------\n",
    "## Overview\n",
    "\n",
    "The primary goal of this submodule is to **generate multiple genome datasets** that will be used in the subsequent submodule on comparative genomics. This will be a short submodule in terms of content, but it is the most computationally intensive submodule.\n",
    "\n",
    "To achieve this, we will **automate the processes outlined in Submodules 1 & 2** using **Nextflow**, a workflow management platform designed to streamline and ensure reproducibility in bioinformatics pipelines. Nextflow allows us to automate the entire workflow, including program version control, ensuring data integrity, and facilitating parallel execution of multiple datasets. This enables us to efficiently run many genome datasets simultaneously, while managing the computational resources available in our environment.\n",
    "\n",
    "After executing the Nextflow workflow on our set of genomes, we will supplement our data with **reference-quality genome sequences from NCBI** in **Submodule 4**.\n",
    "\n",
    "\n",
    "### Learning Objectives\n",
    "- **Learn Reproducibility in Bioinformatics**:  \n",
    "  Understand the importance of reproducibility in bioinformatics workflows and learn how to implement practices that ensure consistent, reliable results across different systems and environments.\n",
    "\n",
    "- **Automate Genome Data Processing**:  \n",
    "  Learn how to automate and streamline the processes of genome assembly, quality control, and annotation to handle multiple genome datasets efficiently using the workflow management platform **Nextflow**.\n",
    "\n",
    "- **Execute Parallel Processing and Ensure Data Integrity**:  \n",
    "  Gain proficiency in managing and processing large sets of genomic data simultaneously, while also developing the ability to monitor, assess, and maintain data integrity and quality throughout the workflow.\n",
    "\n",
    "- **Prepare and Refine Genome Sequences for Comparative Genomics**:  \n",
    "  Refine and prepare a collection of genome datasets, ensuring they are processed, cleaned, and ready for detailed examination in comparative genomics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df288aa-a092-4803-9b2b-ac53da83b7d2",
   "metadata": {},
   "source": [
    "## The Importance of Reproducibility in Bioinformatics\n",
    "\n",
    "In bioinformatics, ensuring that analyses can be repeated with the same results is critical for validating findings. This is especially important given the large datasets and complex analyses often involved, where slight variations in workflows, software versions, or data can lead to different outcomes. \n",
    "\n",
    "To support reproducibility in bioinformatics, many workflows are built using standardized, transparent tools and platforms, such as **Nextflow**, **Snakemake**, and **Galaxy**, which allow scientists to document and automate every step of their analysis. This makes it possible for others to re-run the same pipeline on their own systems, using the same parameters and input data, thus ensuring consistent results.\n",
    "\n",
    "### FAIR Principles\n",
    "\n",
    "The FAIR principles are guidelines designed to enhance the visibility and accessibility of research data. FAIR stands for **Findable, Accessible, Interoperable, and Reusable**. These principles aim to make scientific data easier to find, use, and share, which is especially important in bioinformatics.\n",
    "\n",
    "- **Findable**: Data should be easy to locate through searchable metadata, unique identifiers, and standardized naming conventions.\n",
    "- **Accessible**: Data should be available in formats that are easy to access and retrieve, even if only through proper authentication or permissions.\n",
    "- **Interoperable**: Data and tools should be compatible, allowing for the exchange of information across systems, platforms, and communities.\n",
    "- **Reusable**: Data should be well-documented, with clear descriptions of methods, metadata, and usage licenses, so others can confidently reuse it in future research.\n",
    "\n",
    "### Tools  to Aid in Reproducability in Bioinformatics\n",
    "To ensure reproducibility in bioinformatics workflows, several tools and platforms are available that help automate, document, and track the analysis process. These tools facilitate consistent execution of pipelines, improve collaboration, and ensure that results can be verified and reproduced across different environments and systems.\n",
    "\n",
    "| Tool               | Description | Link |\n",
    "|--------------------|-------------|------|\n",
    "| **Nextflow**       | A workflow management system for running bioinformatics pipelines in a highly scalable, reproducible, and portable manner. | [Nextflow Documentation](https://www.nextflow.io/) |\n",
    "| **RepeatFS**       | A tool designed to automate repetitive computational tasks, similar to Nextflow, for large-scale genomic data analysis. | [RepeatFS GitHub](https://github.com/RepeatFS) |\n",
    "| **Snakemake**      | A popular workflow management system for bioinformatics, known for its simple syntax and scalability. | [Snakemake Documentation](https://snakemake.readthedocs.io/en/stable/) |\n",
    "| **Cromwell**       | A workflow engine designed for scientific workflows, with built-in support for WDL (Workflow Description Language). | [Cromwell GitHub](https://github.com/broadinstitute/cromwell) |\n",
    "| **Galaxy**         | A web-based platform for data-intensive research, offering tools for reproducible data analysis and workflow management. | [Galaxy Project](https://galaxyproject.org/) |\n",
    "| **Airflow**        | An open-source tool for orchestrating complex workflows, with strong scheduling and monitoring features. | [Apache Airflow](https://airflow.apache.org/) |\n",
    "| **Bash / Python**  | Bash and Python scripts can automate tasks, with Bash being simple and flexible for Unix-like systems and Python offering more extensive libraries for bioinformatics tasks. | [Bash Scripting Guide](https://www.gnu.org/software/bash/manual/bash.html), [Python Documentation](https://docs.python.org/3/) |\n",
    "| **Other Languages**| Other programming languages like **R**, **Perl**, and **Ruby** can be used for bioinformatics workflows, depending on the user's needs and preferences. | N/A |\n",
    "| **PipeLine**       | A pipeline management tool that allows for easy tracking and execution of bioinformatics workflows, especially useful in large-scale projects. | [PipeLine GitHub](https://github.com/Pipeline) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62c7ea-5c84-456c-9e96-6a34075f3cab",
   "metadata": {},
   "source": [
    "## **Install required software**\n",
    "\n",
    "The only additional tools required for this submodule is **Nextflow**. All the tools required for the Nextflow workflow have already been installed and described in the previous submodules. To install Nextflow see the official documentation https://www.nextflow.io/docs/latest/install.html. **Nextflow has been installed in the container and does not need to be installed now.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce2ef2-9607-4c73-a5d1-61c813f5741b",
   "metadata": {},
   "source": [
    "## Starting Data\n",
    "As a reminder from Submodule 1, the data used for this module is described in a manuscript comparing phenotypic and whole-genome sequencing-derived AMR profiles ([Painset et al. 2020](https://pubmed.ncbi.nlm.nih.gov/31943013/)). The study includes sequencing read datasets for 528 isolates of Campylobacter spp. (452 C. jejuni and 76 C. coli) from human (494), food (21), and environmental (2) sources.\n",
    "\n",
    "For a robust comparative genomic analysis, it is essential to include a balanced and statistically sufficient sample size for each species. A minimum of 50-100 isolates per species is recommended to capture population-level variability, with larger sample sizes (e.g., 100-200) providing greater statistical power to detect subtle genomic differences.\n",
    "\n",
    "For this tutorial, we will use a small subset of ~10 isolates to ensure analyses run quickly. However, the methods presented here are fully scalable and can be applied to larger datasets like those described in the manuscript. This smaller subset of sequencing data is derived from Table 2. of the manuscript.\n",
    "\n",
    "Lets take a look at the starting dataset. We'll download this data from the NCBI SRA database using the SRA accession column and the same methods we used in Submodule 1.\n",
    "\n",
    "| **Isolate No.** | **SRA Accession** | **Species**  | **Resistance to Erythromycin (Macrolide)** | **Resistance to Ciprofloxacin (Fluoroquinolone)** | **Resistance Determinants (Ciprofloxacin)** | **Resistance to Tetracycline (Tetracycline)** | **Resistance Determinants (Tetracycline)** | **Resistance to Gentamicin (Aminoglycoside)** | **Resistance Determinants (Gentamicin)** | **Resistance to Streptomycin (Aminoglycoside)** | **Resistance Determinants (Streptomycin)** |\n",
    "|------------------|-------------------|--------------|--------------------------------------------|--------------------------------------------------|---------------------------------------------|----------------------------------------------|---------------------------------------------|-----------------------------------------------|-------------------------------------------|------------------------------------------------|---------------------------------------------|\n",
    "| 72               | SRR10067958      | *C. jejuni*  | S                                          | S                                                | —                                           | R                                            | —                                           | S                                             | —                                         | S                                              | —                                           |\n",
    "| 91               | SRR10068079      | *C. jejuni*  | S                                          | S                                                | gyrA_CJ[86:T-I]                             | S                                            | tet(O)                                      | S                                             | —                                         | S                                              | —                                           |\n",
    "| 145              | SRR10068117      | *C. jejuni*  | S                                          | S                                                | —                                           | R                                            | —                                           | S                                             | —                                         | S                                              | —                                           |\n",
    "| 193              | SRR10056681      | *C. jejuni*  | S                                          | S                                                | gyrA_CJ[86:T-I]                             | R                                            | tet(O)_2                                    | S                                             | —                                         | R                                              | —                                           |\n",
    "| 213              | SRR10056829      | *C. jejuni*  | S                                          | S                                                | —                                           | S                                            | tet(O)                                      | S                                             | —                                         | S                                              | ant(6)-Ia, aadE-Cp2                        |\n",
    "| 214              | SRR10056914      | *C. coli*    | S                                          | S                                                | —                                           | S                                            | —                                           | S                                             | —                                         | S                                              | —                                           |\n",
    "| 242              | SRR10056778      | *C. coli*    | S                                          | S                                                | —                                           | S                                            | —                                           | S                                             | —                                         | R                                              | —                                           |\n",
    "| 248              | SRR10056855      | *C. jejuni*  | S                                          | S                                                | 23s_CJ[2074:A-M], gyrA_CJ[86:T-I; 90:D-N]  | R                                            | tet(O)_2                                    | S                                             | —                                         | S                                              | —                                           |\n",
    "| 263              | SRR10056784      | *C. jejuni*  | S                                          | S                                                | gyrA_CJ[86:T-I]                             | R                                            | tet(O)_2                                    | S                                             | —                                         | R                                              | ant(6)-Ia, aadE-Cp2                        |\n",
    "| 276              | SRR10056856      | *C. jejuni*  | S                                          | S                                                | —                                           | S                                            | —                                           | S                                             | —                                         | R                                              | —                                           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833d922-751b-4fb3-b5fb-c9830e1d551d",
   "metadata": {},
   "source": [
    "## Nextflow Overview\n",
    "\n",
    "### Key Features of Nextflow:\n",
    "- **Reproducibility**: Nextflow ensures that workflows are reproducible by capturing the environment, dependencies, and exact command used for each execution.\n",
    "- **Parallelization**: It efficiently runs tasks in parallel, utilizing computational resources optimally, which speeds up large-scale data analyses.\n",
    "- **Compatibility**: Nextflow supports multiple computing environments, including local machines, cloud infrastructure, and high-performance computing clusters.\n",
    "- **Version Control**: Integrated support for managing software versions and dependencies, ensuring consistency across different systems and workflows.\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Resources for Nextflow\n",
    "\n",
    "| Resource | Description | Link |\n",
    "|----------|-------------|------|\n",
    "| **Nextflow Official Documentation** | Comprehensive guide on installation, syntax, and core concepts. | [Nextflow Documentation](https://www.nextflow.io/docs/latest/) |\n",
    "| **Nextflow Tutorials** | Step-by-step tutorials for beginners and advanced users. | [Nextflow Tutorials](https://www.nextflow.io/docs/latest/tutorial/) |\n",
    "| **Nextflow YouTube Channel** | Recorded webinars, conference talks, and demos for visual learners. | [Nextflow YouTube Channel](https://www.youtube.com/c/Nextflow) |\n",
    "| **Nextflow Community Forum** | A forum for discussing issues, asking questions, and sharing knowledge. | [Nextflow Forum](https://www.nextflow.io/community/) |\n",
    "| **Nextflow GitHub Repository** | Access to source code, examples, and community contributions. | [Nextflow GitHub](https://github.com/nextflow-io/nextflow) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2d336b-aa1b-459b-83f4-ea73a5dcefa7",
   "metadata": {},
   "source": [
    "### Download starting read datasets\n",
    "\n",
    "Below are two methods to download starting read datasets. \n",
    "\n",
    "1. Download data from the NCBI SRA using a list of accessions.\n",
    "2. Copying a predownloaded dataset from an AWS S3 bucket. \n",
    "\n",
    "For the standard tutorial, the SRA download is commented out and will not run by default. Within this code, the script *SRA_download.sh* automates the same process we used in Submodule 01 to download data. It iterates through a list of SRA accessions and fetches paired-end sequencing reads for ten samples. If you would like to use the full set of 10 samples with the full number of reads (1.5–3 million per sample), you can comment out (#) the AWS S3 code and remove the comment characters in front of the SRA code. This full dataset will take up to a half hour to run. Also, to use this approach with your own data, simply replace the SRR_list.txt file with a custom list of SRA accessions.\n",
    "\n",
    "The data we pull from the AWS S3 bucket are five samples randomly subsampled down to 300,000 reads to make the NextFlow workflow run in under seven minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47456c32-0bb1-44d3-93af-c0c62a066a91",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <b>Attention: Commented lines below will not run by default. See the note above about the two methods available for acquiring the starting data.</b>  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ddb3e-3660-4a30-bc97-728bef630557",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "read_dir=wgs-nf/raw-reads/\n",
    "mkdir -p $read_dir\n",
    "\n",
    "########################\n",
    "# ### SKIPPING DOWNLOAD FROM SRA, BUT HERE ARE INSTRUCTIONS\n",
    "# # Create list of accessions from manuscript metadata\n",
    "# cat data/metadata.tsv  | grep SRR | awk '{print $2}' > data/SRR_list.txt\n",
    "# cat data/SRR_list.txt\n",
    "\n",
    "# # run a custom BASH script that retrieves the data from NCBI\n",
    "# scripts/SRA_download.sh data/SRR_list.txt $read_dir\n",
    "########################\n",
    "\n",
    "### DOWNLOAD FROM S3 bucket, subset data\n",
    "aws s3 cp s3://nh-inbre-genome-sequencing-and-comparative-genomic-analysis/nextflow-reads/ $read_dir --recursive\n",
    "\n",
    "\n",
    "# list the contents of the starting directory\n",
    "echo \"Directory contents:\"\n",
    "echo \"--------------------\"\n",
    "ls $read_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36441fa-81b3-42ba-9844-3d651e27bfde",
   "metadata": {},
   "source": [
    "## The Nextflow Workflow\n",
    "\n",
    "### Overview\n",
    "\n",
    "Next we will run a bash command that initiates the Nextflow workflow on the directory of paired-end FASTQ files we just downloaded. Each of the sets of FASTQ files represents the sequencing data genearted from a bacterial isolate. The workflow automates all the steps from Submodules 1 and 2, ensuring a seamless process. Nextflow intelligently determines which steps can be executed concurrently (e.g., BLAST and BWA read mapping) and which steps require prior completion of others (e.g., running BlobTools after BLAST and related processes finish). \n",
    "\n",
    "The NextFlow code is written in the file called *main.nf*, I enourage you to open up this file (wgs-nf/main.nf) and examine the code, but we'll examine some of the sections together.\n",
    "\n",
    "### Example of a Nextflow Process: Trimming Paired-End Reads with FASTP\n",
    "Below is a code snippet of the contents of wgs-nf/main.nf as an example of the Nextflow language process for running a command. In this step of the workflow, we trim paired-end sequencing reads using **FASTP**. The `RUN_FASTP` process takes paired-end reads as input and outputs the trimmed versions of the reads. Here's the Nextflow code to perform this task:\n",
    "\n",
    "```nextflow\n",
    "// Step 2: Trim paired-end reads with FASTP\n",
    "process RUN_FASTP {\n",
    "    input:\n",
    "    tuple val(sampleid), path(fq1), path(fq2)\n",
    "    \n",
    "    output:\n",
    "    tuple val(sampleid), path(\"${sampleid}.trimmed_R1.fastq.gz\"), path(\"${sampleid}.trimmed_R2.fastq.gz\")\n",
    "    \n",
    "    script:\n",
    "    \"\"\"\n",
    "    fastp -i $fq1 -I $fq2 \\\n",
    "        -o \"$sampleid\".trimmed_R1.fastq.gz -O \"$sampleid\".trimmed_R2.fastq.gz \\\n",
    "        --thread ${params.threads}\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca625f9-9575-41ff-9472-80c84f8aaa83",
   "metadata": {},
   "source": [
    "### Nextflow Workflow: Putting it all together\n",
    "\n",
    "A similar snippet of code exists for each step of the workflow, with controlled options and threads for each step. After all the processes are written you can execute the code using the workflow block. Below is the Nextflow code for this workflow, this includes steps for read assessment, quality control, genome assembly, and various downstream analyses.\n",
    "\n",
    "```nextflow\n",
    "workflow {\n",
    "    // Load paired-end FASTQ files\n",
    "    fastq_files = Channel\n",
    "    .fromFilePairs(params.reads, flat: false)\n",
    "    .map { sample_id, files -> tuple(sample_id, files[0], files[1]) }\n",
    "\n",
    "    // Step 0: Count reads\n",
    "    read_info = fastq_files | ASSESS_READS\n",
    "    read_info.view()\n",
    "\n",
    "    // Step 1: Run FASTQC\n",
    "    fastqc_report = fastq_files | RUN_FASTQC\n",
    "\n",
    "    // Step 2: Trim reads with FASTP\n",
    "    trimmed_fastq = fastq_files | RUN_FASTP\n",
    "\n",
    "    // Step 3: Assemble genome with SPADES\n",
    "    genome = trimmed_fastq | RUN_SPADES\n",
    "\n",
    "    // Step 4.1: Run BWA and Samtools\n",
    "    bwa_channel = trimmed_fastq.join(genome)\n",
    "    bam = RUN_BWA(bwa_channel)\n",
    "\n",
    "    // Step 4.2: Run BUSCO\n",
    "    busco_results = genome | RUN_BUSCO\n",
    "\n",
    "    // Step 4.3: Run QUAST\n",
    "    quast_report = genome | RUN_QUAST\n",
    "\n",
    "    // Step 4.4: Run BLAST\n",
    "    blast_results = genome | RUN_BLAST\n",
    "\n",
    "    // Step 5: Run Blobtools\n",
    "    busco_channel = genome.join(bam).join(blast_results)\n",
    "    blob_output = RUN_BLOBTOOLS(busco_channel)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e6211c-9911-49e3-ae95-b9146282597b",
   "metadata": {},
   "source": [
    "## Run Nextflow\n",
    "\n",
    "Lets start the process.\n",
    "\n",
    "Note: When running Nextflow inside Jupyter notebooks, the progress updates are printed repeatedly instead of updating cleanly on a single line as they would in a normal terminal. While the output may appear verbose, it contains useful information about the status of the workflow. We will keep this output as-is for the demonstration. You can remove this status bar update using the option *-ansi-log false*. **You can click the left side of the notebook screen to truncate the output feed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a3f56d-9993-44c6-8598-001350d4d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# move into working directory\n",
    "cd wgs-nf/\n",
    "\n",
    "# run nextflow with the continue option\n",
    "nextflow run main.nf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1bee64-9a00-4b3c-8960-2772f5d3cabd",
   "metadata": {},
   "source": [
    "## Nextflow Explanation:\n",
    "\n",
    "As the NextFlow pipeline processed through the steps in the workflow you may have noticed the updates printing to the screen. For example.\n",
    "\n",
    "\n",
    "Example of Nextflow status as the job was running.\n",
    "\n",
    "[hash] PROCESS_NAME (attempt) | completed of total ✔ or pending    \n",
    "\n",
    "[1e/9847bd] ASSESS_READS (5)  | 5 of 5 ✔  \n",
    "[21/55f7a1] RUN_FASTQC (3)    | 5 of 5 ✔  \n",
    "[eb/54aa16] RUN_FASTP (2)     | 5 of 5 ✔  \n",
    "[0d/47a4f6] RUN_SPADES (3)    | 3 of 5  \n",
    "[a8/562c66] RUN_BWA (3)       | 2 of 3  \n",
    "[2a/72a30e] RUN_BUSCO (2)     | 2 of 3  \n",
    "[76/b9a85d] RUN_QUAST (3)     | 2 of 3  \n",
    "[c5/3c7a21] RUN_BLAST (3)     | 3 of 3  \n",
    "[8e/391483] RUN_BLOBTOOLS (2) | 1 of 2  \n",
    "[8c/187cd5] RUN_BAKTA (3)     | 0 of 3  \n",
    "\n",
    "When running a Nextflow pipeline, the status output helps track the progress of each process (step) in the workflow.\n",
    "Each line summarizes a different process:\n",
    "\n",
    "The `PROCESS_NAME` is the current task, such as FASTQC or BUSCO. The number in parentheses reflects how many batches or scheduling groups were created for that process during the workflow. `5 of 5 ✔` means all 5 jobs for this process have successfully completed. \n",
    "\n",
    "You may have noticed that jobs were parallelized, but specific tasks needed to wait to start based on the output from other jobs. For example, SPADES can't run until FASTP finishes generating the trimmed reads. Once the assembly finished, nearly all the other tasks (BUSCO, BWA, QUAST, BAKTA, BLAST) were able to start at the same time. This is one of the most powerful features of the Nextflow language: automatic parallelization and queueing based on dependencies between tasks. This queueing system is sample-dependent and dynamically adapts to your data.\n",
    "\n",
    "### Outputs\n",
    "The workflow cleans up and writes the files we want to save to a final directory called *output-dir*. This main output dircetory contains selected reports from each process (BUSCO results, blobtools plots, etc.). All other work is saved by NextFlow within a directory called *work*. The *hash* ID at the begining of each status details which directory all files are saved.\n",
    "\n",
    "Lets view the primary outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e26c0-fcd5-4cca-a241-a74a63d91512",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <b>Attention:</b> Before You Proceed: Pause here and wait for the nextflow code to finish running. The brackets around the block of code will switch from '*' to a number when it is completed and you will see the duration displayed at the bottom of the output. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64917be4-8cb5-49af-954f-f2119d0183bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# view contents of output directory\n",
    "ls wgs-nf/output-dir/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594d274-3d39-47cc-babe-f4ac51936327",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "### Summarize QUAST results\n",
    "\n",
    "# \n",
    "# cat wgs-nf/output-dir/output-quast/*\n",
    "\n",
    "# genoem sizes\n",
    "echo 'NUMBER OF CONTIGS'\n",
    "grep '# contigs' wgs-nf/output-dir/output-quast/* | grep -v '('\n",
    "\n",
    "# genoem sizes\n",
    "echo 'GENOME ASSEMBLY LENGTH'\n",
    "grep 'Total length' wgs-nf/output-dir/output-quast/* | grep -v '('\n",
    "\n",
    "# N50s\n",
    "echo 'N50s'\n",
    "grep N50 wgs-nf/output-dir/output-quast/*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750492d8-007a-4188-9ad7-bd634a96b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# See the number of complete and missing BUSCOs\n",
    "echo 'Complete BUSCOs:'\n",
    "grep 'single-copy' wgs-nf/output-dir/output-busco/*\n",
    "echo 'Missing BUSCOs:'\n",
    "grep 'Missing' wgs-nf/output-dir/output-busco/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a2e9ef-cf11-447d-84cc-51994451ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# final output directory for the next steps\n",
    "\n",
    "ls wgs-nf/output-dir/proteomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ba0b8-ab2e-4c70-be8e-1caa7524cc94",
   "metadata": {},
   "source": [
    "## End of Submodule03\n",
    "\n",
    "We have successfully automated the workflow we carefully ran in Submodule01 and Submodule02 to produce a bacterial genome for multiple samples. This automated analysis enabled us to run the workflow on many samples in parallel by utilizing the full RAM and CPUs available on our system. The automated approach also enabled us to ensure the same settings were used across our dataset and allows us to reproduce the analysis if needed.\n",
    "\n",
    "With multiple genomes available, we now move into Submodule04 to perform a comparative genomics analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15a6cf8-c095-4d99-986e-bba4896eeb84",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> <b>Attention:</b> Before You Proceed: If you're not moving on immediately to the next submodule, be sure to shutdown your instance. </div>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
